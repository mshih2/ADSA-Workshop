{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to model stacking using sklearn models\n",
    "\n",
    "#### This workshop uses House Price Data from Kaggle to demonstrate some frequently used machine learning techniques and a model stacking and blending appraoch\n",
    "\n",
    "#### Machine Learning Models Covered:\n",
    "**Sklearn:**   \n",
    "Extra Trees   \n",
    "Random Forest   \n",
    "Lasso Regression   \n",
    "Ridge Kernel Regression\n",
    "    \n",
    "**XGBoost:**   \n",
    "XGBoost (Extreme Gradient Boosting)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization: import required packages   \n",
    "Import the data sctructure and analysis packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meichengshih/anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import Lasso\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages related to model ensembling and the other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read cleaned data, the data cleaning process will not be discussed in this workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./train-b.csv',index_col=0)\n",
    "to_pred=pd.read_csv('./test-b.csv',index_col=0)\n",
    "## Read the Log of House Price\n",
    "target=np.log(pd.read_csv('./target.csv',index_col=0, header=None).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define first layer regressors and their corresponding parameters, the parameters were obtained from Bayesian Optimization based parameter tunning process. We will not discuss the details of parameter tuning today. \n",
    "\n",
    "This example will contain 5 first-layer regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Kernel Rdige Model\n",
    "kr=KernelRidge(alpha=10**(-1.75),gamma=10**(-2.25),kernel='rbf')\n",
    "\n",
    "# Define Lasso Model\n",
    "lasso=Lasso(alpha=10**(-3.65),max_iter=3000)\n",
    "\n",
    "# Define XGBoost Model\n",
    "xgbr=xgb.XGBRegressor(colsample_bytree=0.7,gamma=0.02,learning_rate=0.09,max_depth=4, min_child_weight=1,\n",
    "                      n_estimators=420,reg_alpha=0.1,reg_lambda=0.3,subsample=0.75)\n",
    "\n",
    "# Define Extra Trees Regressor\n",
    "extr=ExtraTreesRegressor(max_depth=28,max_features=0.9,n_estimators=450)\n",
    "\n",
    "# Define Random Forest Regressor\n",
    "rfr=RandomForestRegressor(max_depth=25,max_features=0.35,n_estimators=400,min_samples_leaf=1)\n",
    "\n",
    "# Define a list of regressors\n",
    "frgrs=[kr,lasso,xgbr,extr,rfr] # Kernel Ridge, Lasso, XGBoost, Extra Tress and Random Forest Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate performance recorded matrix, and the train and test matrix for the second layer regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define number of folds used for model stacking\n",
    "nf=10\n",
    "\n",
    "# Record the performance of each fold iteration\n",
    "eval_rec=np.zeros((nf,len(frgrs)))\n",
    "\n",
    "# Record the train and test matrix for second layer regressor at the end of each fold iteration\n",
    "blend_train_temp=np.zeros((train.shape[0]))\n",
    "blend_pred_temp=np.zeros((to_pred.shape[0]))\n",
    "\n",
    "# Record the final train and test matrix for the second layer regressor\n",
    "blend_train=np.zeros((train.shape[0],len(frgrs)))\n",
    "blend_pred=np.zeros((to_pred.shape[0],len(frgrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start model stacking process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th Regressor\n",
      "(Fold,RMSE,Time)\n",
      "(1, 0.11540680043610636, 0.1394059658050537)\n",
      "(2, 0.15271535813561726, 0.25655102729797363)\n",
      "(3, 0.13260648681436807, 0.36243295669555664)\n",
      "(4, 0.11905965242209612, 0.4691450595855713)\n",
      "(5, 0.1286724093691877, 0.5749070644378662)\n",
      "(6, 0.13325074762488898, 0.674501895904541)\n",
      "(7, 0.11743053031285458, 0.7860229015350342)\n",
      "(8, 0.13924512444218523, 0.8881549835205078)\n",
      "(9, 0.11017316982513063, 1.000981092453003)\n",
      "(10, 0.11306101255851288, 1.1066110134124756)\n",
      "2th Regressor\n",
      "(Fold,RMSE,Time)\n",
      "(1, 0.11153387832537427, 1.1369519233703613)\n",
      "(2, 0.13569516801538153, 1.1691749095916748)\n",
      "(3, 0.14150482688167307, 1.1972529888153076)\n",
      "(4, 0.10545438209436676, 1.2318739891052246)\n",
      "(5, 0.11346430622036149, 1.2671630382537842)\n",
      "(6, 0.1255775238232511, 1.3090450763702393)\n",
      "(7, 0.10938045794629248, 1.3440499305725098)\n",
      "(8, 0.16128093384880088, 1.3902230262756348)\n",
      "(9, 0.12117078853196341, 1.4256439208984375)\n",
      "(10, 0.10703711963485971, 1.4575998783111572)\n",
      "3th Regressor\n",
      "(Fold,RMSE,Time)\n",
      "(1, 0.11170999385958877, 4.522366046905518)\n",
      "(2, 0.1660415348702996, 7.463269948959351)\n",
      "(3, 0.14531676019555897, 10.449683904647827)\n",
      "(4, 0.11414222086914183, 13.427542924880981)\n",
      "(5, 0.12399928541131389, 16.395094871520996)\n",
      "(6, 0.14359255051345357, 19.375185012817383)\n",
      "(7, 0.11805027219654315, 22.351752996444702)\n",
      "(8, 0.11018224402726953, 25.332036018371582)\n",
      "(9, 0.10304081182450485, 28.324687004089355)\n",
      "(10, 0.11372632118849911, 31.303646087646484)\n",
      "4th Regressor\n",
      "(Fold,RMSE,Time)\n",
      "(1, 0.12658737756392285, 43.64083504676819)\n",
      "(2, 0.17622924513393623, 55.70706605911255)\n",
      "(3, 0.16508893382462969, 67.54260396957397)\n",
      "(4, 0.13453290721213751, 79.36934208869934)\n",
      "(5, 0.13836227026246656, 91.16771101951599)\n",
      "(6, 0.14618033964154248, 103.5900559425354)\n",
      "(7, 0.12473305122325788, 116.40548801422119)\n",
      "(8, 0.12321184845872372, 129.2539939880371)\n",
      "(9, 0.099310984318164208, 141.2639410495758)\n",
      "(10, 0.15108056156822358, 152.69920301437378)\n",
      "5th Regressor\n",
      "(Fold,RMSE,Time)\n",
      "(1, 0.13119318362450832, 157.56946301460266)\n",
      "(2, 0.17363151403913527, 162.39568996429443)\n",
      "(3, 0.15083294387520341, 167.23250007629395)\n",
      "(4, 0.12530527983529685, 172.0594720840454)\n",
      "(5, 0.13989711840653118, 176.9037048816681)\n",
      "(6, 0.14245134099044274, 181.7280650138855)\n",
      "(7, 0.12437679609961896, 186.66398286819458)\n",
      "(8, 0.13007592780746263, 191.80407905578613)\n",
      "(9, 0.099283653530064034, 196.84273099899292)\n",
      "(10, 0.1415099340804315, 202.64544987678528)\n"
     ]
    }
   ],
   "source": [
    "## Record strat time\n",
    "stime=time.time()\n",
    "## K-Fold with Shufffle\n",
    "skf = list(KFold(len(target), nf ,shuffle=True))\n",
    "## Loop over all regressors\n",
    "for j, rgr in enumerate(frgrs):\n",
    "    print (str(j+1)+\"th Regressor\")\n",
    "    print (\"(Fold,RMSE,Time)\")\n",
    "    ## Loop over all folds\n",
    "    for i in xrange(nf):\n",
    "        ### For each fold iteration, determine the train and test data based on the generated fold indices\n",
    "        trainind, testind=skf[i]\n",
    "        xtrain, xtest = train.ix[trainind,:], train.ix[testind,:]\n",
    "        ytrain, ytest = target.ix[trainind], target.ix[testind]\n",
    "        ### Converting y into 1d array \n",
    "        ytrain=ytrain.ix[:,1]\n",
    "        ### Train a model with the train data x and y\n",
    "        rgr.fit(xtrain, ytrain)\n",
    "        ### Predict the y of the test data\n",
    "        ytest_pred = rgr.predict(xtest).astype('float32')\n",
    "        ### Record the predicted y of test data\n",
    "        blend_train_temp[testind]=ytest_pred\n",
    "        ### Predict the y of the data that needed to be predicted\n",
    "        pred = rgr.predict(to_pred).astype('float32')\n",
    "        \n",
    "        ### Record the y of the data that needed to be predicted    \n",
    "        if i==0:\n",
    "            blend_pred_temp=pred\n",
    "        else:\n",
    "            blend_pred_temp=blend_pred_temp+pred\n",
    "        \n",
    "        ### Show the performance of a model at each fold iteration and the total time spent\n",
    "        print (i+1, mean_squared_error(ytest,ytest_pred)**0.5, (time.time()-stime))\n",
    "        ### Record the performance of a model at each fold iteration\n",
    "        eval_rec[i,j]=mean_squared_error(ytest,ytest_pred)**0.5\n",
    "    \n",
    "    ### Record the generated train and predict data from each regressor for the second layer regressor\n",
    "    blend_train[:,j]=blend_train_temp\n",
    "    blend_pred[:,j]=blend_pred_temp/float(nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the average performance of each model under all fold iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12616213,  0.12320994,  0.1249802 ,  0.13853175,  0.13585577])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_rec.mean(axis=0)\n",
    "# 0:Kernel Ridge, 1:Lasso, 2:XGBoost, 3:Extra Tress 4:Random Forest Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the second layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second_rgr=KernelRidge(alpha=10**(-1.35),\n",
    "            gamma=10**(-1.40),\n",
    "            kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the performance of second layer regressor, the blending regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0.10636006887979292, 247.4824459552765)\n",
      "(2, 0.14727981129726095, 247.54855608940125)\n",
      "(3, 0.13021683585698793, 247.6120240688324)\n",
      "(4, 0.10842625802247899, 247.67293000221252)\n",
      "(5, 0.11846726114705514, 247.73842000961304)\n",
      "(6, 0.13492956417904833, 247.7994089126587)\n",
      "(7, 0.10887399094439197, 247.8562150001526)\n",
      "(8, 0.11466587996493099, 247.91720008850098)\n",
      "(9, 0.10641949016692044, 247.97866988182068)\n",
      "(10, 0.10673263252158353, 248.04838109016418)\n"
     ]
    }
   ],
   "source": [
    "### Generate the performance recording matrix\n",
    "sec_eval_rec=np.zeros((nf))\n",
    "\n",
    "for i in xrange(nf):\n",
    "    ### For each fold iteration, determine the train and test data based on the generated fold indices\n",
    "    trainind, testind=skf[i]\n",
    "    xtrain, xtest = blend_train[trainind,:], blend_train[testind,:]\n",
    "    ytrain, ytest = target.ix[trainind], target.ix[testind]\n",
    "    ### Converting y into 1d array \n",
    "    ytrain=ytrain.ix[:,1]\n",
    "    ### Train a model with the train data x and y\n",
    "    second_rgr.fit(xtrain, ytrain)\n",
    "    ### Predict the y of the test data\n",
    "    ytest_pred = second_rgr.predict(xtest).astype('float32')\n",
    "    ### Show the performance of a model at each fold iteration and the total time spent\n",
    "    print (i+1, mean_squared_error(ytest,ytest_pred)**0.5, (time.time()-stime))\n",
    "    ### Record the performance of a model at each fold iteration\n",
    "    sec_eval_rec[i]=mean_squared_error(ytest,ytest_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of second layer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11823717929804514"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_eval_rec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the performances with a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "perf=np.append(eval_rec.mean(axis=0),(sec_eval_rec.mean(axis=0)))\n",
    "index = np.arange(len(perf))\n",
    "objects = ('KR', 'Lasso', 'XGB', 'Extra Tress', 'RF', 'Sec_KR')\n",
    "plt.bar(index, perf, align='center', alpha=0.5)\n",
    "plt.xticks(index, objects)\n",
    "plt.ylabel('RMSE')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.11, 0.14])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Lasso as the second layer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "second_rgr2=Lasso(alpha=10**(-3.35),max_iter=3000)\n",
    "\n",
    "### Generate the performance recording matrix\n",
    "lasso_sec_eval_rec=np.zeros((nf))\n",
    "\n",
    "for i in xrange(nf):\n",
    "    ### For each fold iteration, determine the train and test data based on the generated fold indices\n",
    "    trainind, testind=skf[i]\n",
    "    xtrain, xtest = blend_train[trainind,:], blend_train[testind,:]\n",
    "    ytrain, ytest = target.ix[trainind], target.ix[testind]\n",
    "    ### Converting y into 1d array \n",
    "    ytrain=ytrain.ix[:,1]\n",
    "    ### Train a model with the train data x and y\n",
    "    second_rgr2.fit(xtrain, ytrain)\n",
    "    ### Predict the y of the test data\n",
    "    ytest_pred = second_rgr2.predict(xtest).astype('float32')\n",
    "    ### Show the performance of a model at each fold iteration and the total time spent\n",
    "    print (i+1, mean_squared_error(ytest,ytest_pred)**0.5, (time.time()-stime))\n",
    "    ### Record the performance of a model at each fold iteration\n",
    "    lasso_sec_eval_rec[i]=mean_squared_error(ytest,ytest_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lasso_sec_eval_rec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using XGBoost as the second layer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "second_rgr3=xgb.XGBRegressor(colsample_bytree=0.35,gamma=0.085,learning_rate=0.12,max_depth=5, min_child_weight=5.75,\n",
    "                      n_estimators=520,reg_alpha=0.6,reg_lambda=4.75,subsample=0.25)\n",
    "\n",
    "### Generate the performance recording matrix\n",
    "xgb_sec_eval_rec=np.zeros((nf))\n",
    "\n",
    "for i in xrange(nf):\n",
    "    ### For each fold iteration, determine the train and test data based on the generated fold indices\n",
    "    trainind, testind=skf[i]\n",
    "    xtrain, xtest = blend_train[trainind,:], blend_train[testind,:]\n",
    "    ytrain, ytest = target.ix[trainind], target.ix[testind]\n",
    "    ### Converting y into 1d array \n",
    "    ytrain=ytrain.ix[:,1]\n",
    "    ### Train a model with the train data x and y\n",
    "    second_rgr3.fit(xtrain, ytrain)\n",
    "    ### Predict the y of the test data\n",
    "    ytest_pred = second_rgr3.predict(xtest).astype('float32')\n",
    "    ### Show the performance of a model at each fold iteration and the total time spent\n",
    "    print (i+1, mean_squared_error(ytest,ytest_pred)**0.5, (time.time()-stime))\n",
    "    ### Record the performance of a model at each fold iteration\n",
    "    xgb_sec_eval_rec[i]=mean_squared_error(ytest,ytest_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_sec_eval_rec.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between the difference models as the second layer regressor, the red line is the best performance of first layer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "perf=np.append((sec_eval_rec.mean(axis=0)),(lasso_sec_eval_rec.mean(axis=0)))\n",
    "perf=np.append(perf,(xgb_sec_eval_rec.mean(axis=0)))\n",
    "index = np.arange(len(perf))\n",
    "objects = ('Sec_KR', 'Sec_Lasso', 'Sec_XGB')\n",
    "plt.bar(index, perf, align='center', alpha=0.5)\n",
    "plt.xticks(index, objects)\n",
    "plt.ylabel('RMSE')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.11, 0.125])\n",
    "plt.axhline(y=0.12314992, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reason why KR performs better than Lasso and XGB as the second layer regressor is because KR can deal with the homogeneious inputs better.  Based on this idea, the use of Neural Network as the second layer  is expected to generate a good quality of output too."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
